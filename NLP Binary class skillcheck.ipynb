{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c48fdc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\shaik\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c6c918dea3432e83f3e3a82c7c3e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05fa2bebb7e945ff95c0913177a74f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\shaik\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0.incompleteM8DDF6\\imdb_reviews-train…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\shaik\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0.incompleteM8DDF6\\imdb_reviews-test.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\shaik\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0.incompleteM8DDF6\\imdb_reviews-unsup…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Dataset is using deprecated text encoder API which will be removed soon. Please use the plain_text version of the dataset and migrate to `tensorflow_text`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\shaik\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers,Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D,Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pathlib\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import tensorflow_datasets as tfds\n",
    "imdb, info = tfds.load(\"imdb_reviews/subwords8k\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c0d5862",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateReducerCb(tf.keras.callbacks.Callback):\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    old_lr = self.model.optimizer.lr.read_value()\n",
    "    new_lr = old_lr * 0.99\n",
    "    print(\"\\nEpoch: {}. Reducing Learning Rate from {} to {}\".format(epoch, old_lr, new_lr))\n",
    "    self.model.optimizer.lr.assign(new_lr)\n",
    "    \n",
    "def create_checkpoint_callback(model_name):\n",
    "  return tf.keras.callbacks.ModelCheckpoint(filepath=f\"{model_name}/checkpoint.ckpt\",\n",
    "                                                          #  monitor=\"val_accuracy\",\n",
    "                                                           save_best_only=True,\n",
    "                                                           save_weights_only=True,\n",
    "                                                           save_freq=\"epoch\")\n",
    "\n",
    "es_cb = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af646f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = imdb['train'], imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "786deaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "380de801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [6307, 2327, 4043, 2120, 2, 48, 4249, 4429, 7, 2652, 8050]\n",
      "The original string: TensorFlow, from basics to mastery\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'TensorFlow, from basics to mastery'\n",
    "\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "678d0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "\n",
    "train_batches = (\n",
    "    train_data\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .padded_batch(32))\n",
    "\n",
    "test_batches = (\n",
    "    test_data\n",
    "    .padded_batch(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15d92daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "739/782 [===========================>..] - ETA: 53s - loss: 0.6005 - accuracy: 0.6684"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16704/2367131904.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m model_history = model.fit(train_batches,\n\u001b[0m\u001b[0;32m     17\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_batches\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "# Fit the model\n",
    "model_history = model.fit(train_batches,\n",
    "                              validation_data=test_batches,\n",
    "                              epochs=10,\n",
    "                              callbacks=[create_checkpoint_callback(model.name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),\n",
    "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "# Fit the model\n",
    "model_history = model.fit(train_batches,\n",
    "                              validation_data=test_batches,\n",
    "                              epochs=10,\n",
    "                              callbacks=[create_checkpoint_callback(model.name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945d1ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.vocab_size, embedding_dim),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "# Fit the model\n",
    "model_history = model.fit(train_batches,\n",
    "                              validation_data=test_batches,\n",
    "                              epochs=10,\n",
    "                              callbacks=[create_checkpoint_callback(model.name)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585edf6d",
   "metadata": {},
   "source": [
    "# Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "438b0166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26704</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/american-...</td>\n",
       "      <td>american politics in moral free-fall</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26705</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/americas-...</td>\n",
       "      <td>america's best 20 hikes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26706</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/reparatio...</td>\n",
       "      <td>reparations and obama</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26707</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/israeli-b...</td>\n",
       "      <td>israeli ban targeting boycott supporters raise...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26708</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/gourmet-g...</td>\n",
       "      <td>gourmet gifts for the foodie 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26709 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            article_link  \\\n",
       "0      https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1      https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2      https://local.theonion.com/mom-starting-to-fea...   \n",
       "3      https://politics.theonion.com/boehner-just-wan...   \n",
       "4      https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "...                                                  ...   \n",
       "26704  https://www.huffingtonpost.com/entry/american-...   \n",
       "26705  https://www.huffingtonpost.com/entry/americas-...   \n",
       "26706  https://www.huffingtonpost.com/entry/reparatio...   \n",
       "26707  https://www.huffingtonpost.com/entry/israeli-b...   \n",
       "26708  https://www.huffingtonpost.com/entry/gourmet-g...   \n",
       "\n",
       "                                                headline  is_sarcastic  \n",
       "0      former versace store clerk sues over secret 'b...             0  \n",
       "1      the 'roseanne' revival catches up to our thorn...             0  \n",
       "2      mom starting to fear son's web series closest ...             1  \n",
       "3      boehner just wants wife to listen, not come up...             1  \n",
       "4      j.k. rowling wishes snape happy birthday in th...             0  \n",
       "...                                                  ...           ...  \n",
       "26704               american politics in moral free-fall             0  \n",
       "26705                            america's best 20 hikes             0  \n",
       "26706                              reparations and obama             0  \n",
       "26707  israeli ban targeting boycott supporters raise...             0  \n",
       "26708                  gourmet gifts for the foodie 2014             0  \n",
       "\n",
       "[26709 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_json('Sarcasm_Headlines_Dataset.json',lines=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52f1cdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26704</th>\n",
       "      <td>american politics in moral free-fall</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26705</th>\n",
       "      <td>america's best 20 hikes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26706</th>\n",
       "      <td>reparations and obama</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26707</th>\n",
       "      <td>israeli ban targeting boycott supporters raise...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26708</th>\n",
       "      <td>gourmet gifts for the foodie 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26709 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  is_sarcastic\n",
       "0      former versace store clerk sues over secret 'b...             0\n",
       "1      the 'roseanne' revival catches up to our thorn...             0\n",
       "2      mom starting to fear son's web series closest ...             1\n",
       "3      boehner just wants wife to listen, not come up...             1\n",
       "4      j.k. rowling wishes snape happy birthday in th...             0\n",
       "...                                                  ...           ...\n",
       "26704               american politics in moral free-fall             0\n",
       "26705                            america's best 20 hikes             0\n",
       "26706                              reparations and obama             0\n",
       "26707  israeli ban targeting boycott supporters raise...             0\n",
       "26708                  gourmet gifts for the foodie 2014             0\n",
       "\n",
       "[26709 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(['article_link'],axis=1,inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0084086f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6913</th>\n",
       "      <td>isis recruiter excited to be talking to popula...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11611</th>\n",
       "      <td>jimmy fallon could barely keep it together dur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25947</th>\n",
       "      <td>drunk ron weasley wishing harry potter 'happy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24478</th>\n",
       "      <td>hoosier hostility: not the american way</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11863</th>\n",
       "      <td>turns out running doesn't wreck your knees aft...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  is_sarcastic\n",
       "6913   isis recruiter excited to be talking to popula...             1\n",
       "11611  jimmy fallon could barely keep it together dur...             0\n",
       "25947  drunk ron weasley wishing harry potter 'happy ...             0\n",
       "24478            hoosier hostility: not the american way             0\n",
       "11863  turns out running doesn't wreck your knees aft...             0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_shuffled = data.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility\n",
    "data_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9898e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21367, 5342, 21367, 5342)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(data_shuffled[\"headline\"].to_numpy(),\n",
    "                                                                            data_shuffled[\"is_sarcastic\"].to_numpy(),\n",
    "                                                                            test_size=0.2,\n",
    "                                                                            random_state=42)\n",
    "\n",
    "len(train_sentences), len(val_sentences), len(train_labels), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d30fdd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2fc150d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup text vectorization with custom variables\n",
    "max_vocab_length = 1000 # max number of words to have in our vocabulary\n",
    "max_length = 10 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length, # how many words in the vocabulary (all of the different words in your text)\n",
    "                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
    "                                    split=\"whitespace\", # how to split tokens\n",
    "                                    ngrams=None, # create groups of n-words?\n",
    "                                    output_mode=\"int\", # how to map tokens to numbers\n",
    "                                    output_sequence_length=max_length) # how long should the output sequence of tokens be?\n",
    "                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "95cb52ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4233dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
    "                             output_dim=128, # set size of embedding vector\n",
    "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
    "                             input_length=max_length, # how long is each input\n",
    "                             name=\"embedding_1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cc3369fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "665/668 [============================>.] - ETA: 0s - loss: 0.3351 - accuracy: 0.8512\n",
      "Epoch: 0. Reducing Learning Rate from 0.0010000000474974513 to 0.0009900000877678394\n",
      "668/668 [==============================] - 13s 16ms/step - loss: 0.3351 - accuracy: 0.8512 - val_loss: 0.4071 - val_accuracy: 0.8270\n",
      "Epoch 2/10\n",
      "667/668 [============================>.] - ETA: 0s - loss: 0.2689 - accuracy: 0.8787\n",
      "Epoch: 1. Reducing Learning Rate from 0.0009900000877678394 to 0.000980100128799677\n",
      "668/668 [==============================] - 10s 15ms/step - loss: 0.2689 - accuracy: 0.8787 - val_loss: 0.4336 - val_accuracy: 0.8179\n",
      "Epoch 3/10\n",
      "665/668 [============================>.] - ETA: 0s - loss: 0.2441 - accuracy: 0.8910\n",
      "Epoch: 2. Reducing Learning Rate from 0.000980100128799677 to 0.0009702991228550673\n",
      "668/668 [==============================] - 10s 15ms/step - loss: 0.2441 - accuracy: 0.8910 - val_loss: 0.4543 - val_accuracy: 0.8205\n",
      "Epoch 4/10\n",
      "668/668 [==============================] - ETA: 0s - loss: 0.2260 - accuracy: 0.8987\n",
      "Epoch: 3. Reducing Learning Rate from 0.0009702991228550673 to 0.0009605961386114359\n",
      "668/668 [==============================] - 10s 15ms/step - loss: 0.2260 - accuracy: 0.8987 - val_loss: 0.4855 - val_accuracy: 0.8109\n",
      "Epoch 5/10\n",
      "665/668 [============================>.] - ETA: 0s - loss: 0.2074 - accuracy: 0.9082\n",
      "Epoch: 4. Reducing Learning Rate from 0.0009605961386114359 to 0.0009509901865385473\n",
      "668/668 [==============================] - 13s 20ms/step - loss: 0.2074 - accuracy: 0.9082 - val_loss: 0.5220 - val_accuracy: 0.8124\n",
      "Epoch 6/10\n",
      "668/668 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9152\n",
      "Epoch: 5. Reducing Learning Rate from 0.0009509901865385473 to 0.0009414802771061659\n",
      "668/668 [==============================] - 11s 17ms/step - loss: 0.1927 - accuracy: 0.9152 - val_loss: 0.5491 - val_accuracy: 0.8014\n",
      "Epoch 7/10\n",
      "665/668 [============================>.] - ETA: 0s - loss: 0.1796 - accuracy: 0.9223\n",
      "Epoch: 6. Reducing Learning Rate from 0.0009414802771061659 to 0.0009320654789917171\n",
      "668/668 [==============================] - 12s 19ms/step - loss: 0.1795 - accuracy: 0.9224 - val_loss: 0.6332 - val_accuracy: 0.8113\n",
      "Epoch 8/10\n",
      "665/668 [============================>.] - ETA: 0s - loss: 0.1675 - accuracy: 0.9252\n",
      "Epoch: 7. Reducing Learning Rate from 0.0009320654789917171 to 0.0009227448608726263\n",
      "668/668 [==============================] - 12s 18ms/step - loss: 0.1678 - accuracy: 0.9252 - val_loss: 0.6563 - val_accuracy: 0.8066\n",
      "Epoch 9/10\n",
      "667/668 [============================>.] - ETA: 0s - loss: 0.1552 - accuracy: 0.9312\n",
      "Epoch: 8. Reducing Learning Rate from 0.0009227448608726263 to 0.000913517433218658\n",
      "668/668 [==============================] - 10s 15ms/step - loss: 0.1552 - accuracy: 0.9312 - val_loss: 0.6703 - val_accuracy: 0.8059\n",
      "Epoch 10/10\n",
      "668/668 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9367\n",
      "Epoch: 9. Reducing Learning Rate from 0.000913517433218658 to 0.0009043822647072375\n",
      "668/668 [==============================] - 11s 16ms/step - loss: 0.1440 - accuracy: 0.9367 - val_loss: 0.7671 - val_accuracy: 0.8046\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"input_layer\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.LSTM(64, return_sequences=True)(x)\n",
    "x = layers.LSTM(64)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs, name=\"model_LSTM\")\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "# Fit the model\n",
    "model_history = model.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              epochs=10,\n",
    "                              callbacks=[create_checkpoint_callback(model.name),LearningRateReducerCb()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5bbe12ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "668/668 [==============================] - 7s 9ms/step - loss: 0.3669 - accuracy: 0.8339 - val_loss: 0.4051 - val_accuracy: 0.8091\n",
      "Epoch 2/10\n",
      "668/668 [==============================] - 6s 9ms/step - loss: 0.3151 - accuracy: 0.8581 - val_loss: 0.4050 - val_accuracy: 0.8179\n",
      "Epoch 3/10\n",
      "668/668 [==============================] - 6s 9ms/step - loss: 0.2842 - accuracy: 0.8734 - val_loss: 0.4289 - val_accuracy: 0.8184\n",
      "Epoch 4/10\n",
      "668/668 [==============================] - 6s 9ms/step - loss: 0.2530 - accuracy: 0.8912 - val_loss: 0.4404 - val_accuracy: 0.8160\n",
      "Epoch 5/10\n",
      "668/668 [==============================] - 6s 9ms/step - loss: 0.2235 - accuracy: 0.9052 - val_loss: 0.5132 - val_accuracy: 0.8074\n",
      "Epoch 6/10\n",
      "668/668 [==============================] - 5s 8ms/step - loss: 0.1901 - accuracy: 0.9191 - val_loss: 0.5537 - val_accuracy: 0.8063\n",
      "Epoch 7/10\n",
      "668/668 [==============================] - 6s 9ms/step - loss: 0.1632 - accuracy: 0.9319 - val_loss: 0.6238 - val_accuracy: 0.8034\n",
      "Epoch 8/10\n",
      "668/668 [==============================] - 6s 9ms/step - loss: 0.1395 - accuracy: 0.9412 - val_loss: 0.7213 - val_accuracy: 0.8051\n",
      "Epoch 9/10\n",
      "668/668 [==============================] - 6s 9ms/step - loss: 0.1189 - accuracy: 0.9508 - val_loss: 0.7962 - val_accuracy: 0.7943\n",
      "Epoch 10/10\n",
      "668/668 [==============================] - 6s 9ms/step - loss: 0.1022 - accuracy: 0.9575 - val_loss: 0.9545 - val_accuracy: 0.8049\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"input_layer\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x= layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs, name=\"model_LSTM\")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "# Fit the model\n",
    "model_history = model.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              epochs=10,\n",
    "                              callbacks=[create_checkpoint_callback(model.name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2928529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "660/668 [============================>.] - ETA: 0s - loss: 0.4770 - accuracy: 0.7879\n",
      "Epoch: 0. Reducing Learning Rate from 0.0010000000474974513 to 0.0009900000877678394\n",
      "668/668 [==============================] - 5s 6ms/step - loss: 0.4763 - accuracy: 0.7885 - val_loss: 0.3700 - val_accuracy: 0.8439\n",
      "Epoch 2/10\n",
      "664/668 [============================>.] - ETA: 0s - loss: 0.3111 - accuracy: 0.8713\n",
      "Epoch: 1. Reducing Learning Rate from 0.0009900000877678394 to 0.000980100128799677\n",
      "668/668 [==============================] - 4s 6ms/step - loss: 0.3109 - accuracy: 0.8714 - val_loss: 0.3556 - val_accuracy: 0.8446\n",
      "Epoch 3/10\n",
      "660/668 [============================>.] - ETA: 0s - loss: 0.2661 - accuracy: 0.8921\n",
      "Epoch: 2. Reducing Learning Rate from 0.000980100128799677 to 0.0009702991228550673\n",
      "668/668 [==============================] - 4s 6ms/step - loss: 0.2663 - accuracy: 0.8921 - val_loss: 0.3646 - val_accuracy: 0.8448\n",
      "Epoch 4/10\n",
      "663/668 [============================>.] - ETA: 0s - loss: 0.2448 - accuracy: 0.9001\n",
      "Epoch: 3. Reducing Learning Rate from 0.0009702991228550673 to 0.0009605961386114359\n",
      "668/668 [==============================] - 4s 6ms/step - loss: 0.2450 - accuracy: 0.9000 - val_loss: 0.3818 - val_accuracy: 0.8386\n",
      "Epoch 5/10\n",
      "662/668 [============================>.] - ETA: 0s - loss: 0.2303 - accuracy: 0.9066\n",
      "Epoch: 4. Reducing Learning Rate from 0.0009605961386114359 to 0.0009509901865385473\n",
      "668/668 [==============================] - 4s 6ms/step - loss: 0.2303 - accuracy: 0.9067 - val_loss: 0.4035 - val_accuracy: 0.8377\n",
      "Epoch 6/10\n",
      "661/668 [============================>.] - ETA: 0s - loss: 0.2204 - accuracy: 0.9110\n",
      "Epoch: 5. Reducing Learning Rate from 0.0009509901865385473 to 0.0009414802771061659\n",
      "668/668 [==============================] - 4s 6ms/step - loss: 0.2201 - accuracy: 0.9109 - val_loss: 0.4243 - val_accuracy: 0.8340\n",
      "Epoch 7/10\n",
      "660/668 [============================>.] - ETA: 0s - loss: 0.2119 - accuracy: 0.9138\n",
      "Epoch: 6. Reducing Learning Rate from 0.0009414802771061659 to 0.0009320654789917171\n",
      "668/668 [==============================] - 4s 6ms/step - loss: 0.2124 - accuracy: 0.9136 - val_loss: 0.4437 - val_accuracy: 0.8298\n",
      "Epoch 8/10\n",
      "667/668 [============================>.] - ETA: 0s - loss: 0.2062 - accuracy: 0.9180\n",
      "Epoch: 7. Reducing Learning Rate from 0.0009320654789917171 to 0.0009227448608726263\n",
      "668/668 [==============================] - 4s 6ms/step - loss: 0.2062 - accuracy: 0.9181 - val_loss: 0.4645 - val_accuracy: 0.8276\n",
      "Epoch 9/10\n",
      "664/668 [============================>.] - ETA: 0s - loss: 0.2017 - accuracy: 0.9189\n",
      "Epoch: 8. Reducing Learning Rate from 0.0009227448608726263 to 0.000913517433218658\n",
      "668/668 [==============================] - 4s 6ms/step - loss: 0.2012 - accuracy: 0.9190 - val_loss: 0.4817 - val_accuracy: 0.8268\n",
      "Epoch 10/10\n",
      "661/668 [============================>.] - ETA: 0s - loss: 0.1981 - accuracy: 0.9208\n",
      "Epoch: 9. Reducing Learning Rate from 0.000913517433218658 to 0.0009043822647072375\n",
      "668/668 [==============================] - 4s 6ms/step - loss: 0.1977 - accuracy: 0.9207 - val_loss: 0.4974 - val_accuracy: 0.8216\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"input_layer\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs, name=\"model_LSTM\")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "# Fit the model\n",
    "model_history = model.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              epochs=10,\n",
    "                              callbacks=[create_checkpoint_callback(model.name),es_cb,LearningRateReducerCb()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4af7e464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "668/668 [==============================] - ETA: 0s - loss: 0.3087 - accuracy: 0.8639\n",
      "Epoch: 0. Reducing Learning Rate from 0.0010000000474974513 to 0.0009900000877678394\n",
      "668/668 [==============================] - 21s 26ms/step - loss: 0.3087 - accuracy: 0.8639 - val_loss: 0.4552 - val_accuracy: 0.7956\n",
      "Epoch 2/10\n",
      "666/668 [============================>.] - ETA: 0s - loss: 0.2346 - accuracy: 0.8968\n",
      "Epoch: 1. Reducing Learning Rate from 0.0009900000877678394 to 0.000980100128799677\n",
      "668/668 [==============================] - 16s 24ms/step - loss: 0.2344 - accuracy: 0.8969 - val_loss: 0.4752 - val_accuracy: 0.8184\n",
      "Epoch 3/10\n",
      "667/668 [============================>.] - ETA: 0s - loss: 0.1976 - accuracy: 0.9150\n",
      "Epoch: 2. Reducing Learning Rate from 0.000980100128799677 to 0.0009702991228550673\n",
      "668/668 [==============================] - 16s 24ms/step - loss: 0.1975 - accuracy: 0.9150 - val_loss: 0.5484 - val_accuracy: 0.8152\n",
      "Epoch 4/10\n",
      "667/668 [============================>.] - ETA: 0s - loss: 0.1722 - accuracy: 0.9256\n",
      "Epoch: 3. Reducing Learning Rate from 0.0009702991228550673 to 0.0009605961386114359\n",
      "668/668 [==============================] - 16s 24ms/step - loss: 0.1722 - accuracy: 0.9255 - val_loss: 0.6534 - val_accuracy: 0.8195\n",
      "Epoch 5/10\n",
      "666/668 [============================>.] - ETA: 0s - loss: 0.1544 - accuracy: 0.9320\n",
      "Epoch: 4. Reducing Learning Rate from 0.0009605961386114359 to 0.0009509901865385473\n",
      "668/668 [==============================] - 16s 24ms/step - loss: 0.1546 - accuracy: 0.9320 - val_loss: 0.7231 - val_accuracy: 0.8147\n",
      "Epoch 6/10\n",
      "667/668 [============================>.] - ETA: 0s - loss: 0.1426 - accuracy: 0.9354\n",
      "Epoch: 5. Reducing Learning Rate from 0.0009509901865385473 to 0.0009414802771061659\n",
      "668/668 [==============================] - 16s 24ms/step - loss: 0.1425 - accuracy: 0.9354 - val_loss: 0.7688 - val_accuracy: 0.8083\n",
      "Epoch 7/10\n",
      "668/668 [==============================] - ETA: 0s - loss: 0.1308 - accuracy: 0.9416\n",
      "Epoch: 6. Reducing Learning Rate from 0.0009414802771061659 to 0.0009320654789917171\n",
      "668/668 [==============================] - 16s 24ms/step - loss: 0.1308 - accuracy: 0.9416 - val_loss: 0.8301 - val_accuracy: 0.8079\n",
      "Epoch 8/10\n",
      "666/668 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9461\n",
      "Epoch: 7. Reducing Learning Rate from 0.0009320654789917171 to 0.0009227448608726263\n",
      "668/668 [==============================] - 16s 24ms/step - loss: 0.1195 - accuracy: 0.9461 - val_loss: 0.8506 - val_accuracy: 0.8100\n",
      "Epoch 9/10\n",
      "666/668 [============================>.] - ETA: 0s - loss: 0.1106 - accuracy: 0.9497\n",
      "Epoch: 8. Reducing Learning Rate from 0.0009227448608726263 to 0.000913517433218658\n",
      "668/668 [==============================] - 16s 24ms/step - loss: 0.1106 - accuracy: 0.9496 - val_loss: 0.8746 - val_accuracy: 0.8081\n",
      "Epoch 10/10\n",
      "667/668 [============================>.] - ETA: 0s - loss: 0.1051 - accuracy: 0.9513\n",
      "Epoch: 9. Reducing Learning Rate from 0.000913517433218658 to 0.0009043822647072375\n",
      "668/668 [==============================] - 24s 36ms/step - loss: 0.1052 - accuracy: 0.9512 - val_loss: 0.9488 - val_accuracy: 0.8051\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"input_layer\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(tf.keras.layers.LSTM(32))(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs, name=\"model_LSTM\")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "# Fit the model\n",
    "model_history = model.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              epochs=10,\n",
    "                              callbacks=[create_checkpoint_callback(model.name),es_cb,LearningRateReducerCb()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01ab3c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, None, 64)          523840    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 64)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 524,237\n",
      "Trainable params: 524,237\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
