{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bcc9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_checkpoint_callback(model_name):\n",
    "    return tf.keras.callbacks.ModelCheckpoint(filepath=f\"{model_name}/checkpoint.ckpt\",\n",
    "                                              #  monitor=\"val_accuracy\",\n",
    "                                              save_best_only=True,\n",
    "                                              save_weights_only=True,\n",
    "                                              save_freq=\"epoch\")\n",
    "\n",
    "# Note that you'll need to save your model as a .h5 like this.\n",
    "# When you press the Submit and Test button, your saved .h5 model will\n",
    "# be sent to the testing infrastructure for scoring\n",
    "# and the score will be returned to you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9bb1073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution_model():\n",
    "    xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "    ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
    "    xs_train=xs[:-2]\n",
    "    ys_train=ys[:-2]\n",
    "    xs_test=xs[-2:]\n",
    "    ys_test=ys[-2:]\n",
    "    # YOUR CODE HERE\n",
    "    tf.random.set_seed(42)\n",
    "    model_1 = tf.keras.Sequential([\n",
    "        layers.Dense(10),\n",
    "        layers.Dense(1)\n",
    "    ], name=\"model_1_dense\")\n",
    "    # Compile the model\n",
    "    model_1.compile(\n",
    "        loss=\"mae\",\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    # Fit the model\n",
    "    model_1_history = model_1.fit(tf.expand_dims(xs_train,axis=-1),\n",
    "                                  tf.expand_dims(ys_train,axis=-1),\n",
    "                                  validation_data=(tf.expand_dims(xs_test,axis=-1),\n",
    "                                  tf.expand_dims(ys_test,axis=-1)),\n",
    "                                  epochs=300,callbacks=[create_checkpoint_callback(model_1.name)])\n",
    "    model_1.load_weights(\"model_1_dense/checkpoint.ckpt\")\n",
    "    return model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a91939d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 1.6285 - mae: 1.6285 - val_loss: 4.6759 - val_mae: 4.6759\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.6217 - mae: 1.6217 - val_loss: 4.6520 - val_mae: 4.6520\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.6149 - mae: 1.6149 - val_loss: 4.6280 - val_mae: 4.6280\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.6080 - mae: 1.6080 - val_loss: 4.6041 - val_mae: 4.6041\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.6012 - mae: 1.6012 - val_loss: 4.5800 - val_mae: 4.5800\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.5943 - mae: 1.5943 - val_loss: 4.5560 - val_mae: 4.5560\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.5874 - mae: 1.5874 - val_loss: 4.5319 - val_mae: 4.5319\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.5805 - mae: 1.5805 - val_loss: 4.5078 - val_mae: 4.5078\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.5737 - mae: 1.5737 - val_loss: 4.4837 - val_mae: 4.4837\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.5668 - mae: 1.5668 - val_loss: 4.4595 - val_mae: 4.4595\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.5599 - mae: 1.5599 - val_loss: 4.4353 - val_mae: 4.4353\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.5529 - mae: 1.5529 - val_loss: 4.4110 - val_mae: 4.4110\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.5460 - mae: 1.5460 - val_loss: 4.3868 - val_mae: 4.3868\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.5391 - mae: 1.5391 - val_loss: 4.3624 - val_mae: 4.3624\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.5321 - mae: 1.5321 - val_loss: 4.3381 - val_mae: 4.3381\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.5252 - mae: 1.5252 - val_loss: 4.3137 - val_mae: 4.3137\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.5182 - mae: 1.5182 - val_loss: 4.2893 - val_mae: 4.2893\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.5112 - mae: 1.5112 - val_loss: 4.2648 - val_mae: 4.2648\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.5042 - mae: 1.5042 - val_loss: 4.2403 - val_mae: 4.2403\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.4972 - mae: 1.4972 - val_loss: 4.2158 - val_mae: 4.2158\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.4902 - mae: 1.4902 - val_loss: 4.1912 - val_mae: 4.1912\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.4832 - mae: 1.4832 - val_loss: 4.1666 - val_mae: 4.1666\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.4762 - mae: 1.4762 - val_loss: 4.1419 - val_mae: 4.1419\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.4691 - mae: 1.4691 - val_loss: 4.1172 - val_mae: 4.1172\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.4621 - mae: 1.4621 - val_loss: 4.0925 - val_mae: 4.0925\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.4550 - mae: 1.4550 - val_loss: 4.0677 - val_mae: 4.0677\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.4479 - mae: 1.4479 - val_loss: 4.0429 - val_mae: 4.0429\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.4408 - mae: 1.4408 - val_loss: 4.0180 - val_mae: 4.0180\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.4337 - mae: 1.4337 - val_loss: 3.9931 - val_mae: 3.9931\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.4266 - mae: 1.4266 - val_loss: 3.9681 - val_mae: 3.9681\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.4195 - mae: 1.4195 - val_loss: 3.9431 - val_mae: 3.9431\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.4123 - mae: 1.4123 - val_loss: 3.9181 - val_mae: 3.9181\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.4052 - mae: 1.4052 - val_loss: 3.8930 - val_mae: 3.8930\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.3980 - mae: 1.3980 - val_loss: 3.8678 - val_mae: 3.8678\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.3908 - mae: 1.3908 - val_loss: 3.8426 - val_mae: 3.8426\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.3836 - mae: 1.3836 - val_loss: 3.8174 - val_mae: 3.8174\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.3764 - mae: 1.3764 - val_loss: 3.7921 - val_mae: 3.7921\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.3692 - mae: 1.3692 - val_loss: 3.7668 - val_mae: 3.7668\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.3619 - mae: 1.3619 - val_loss: 3.7414 - val_mae: 3.7414\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.3547 - mae: 1.3547 - val_loss: 3.7160 - val_mae: 3.7160\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.3474 - mae: 1.3474 - val_loss: 3.6905 - val_mae: 3.6905\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.3401 - mae: 1.3401 - val_loss: 3.6649 - val_mae: 3.6649\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.3328 - mae: 1.3328 - val_loss: 3.6394 - val_mae: 3.6394\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.3255 - mae: 1.3255 - val_loss: 3.6137 - val_mae: 3.6137\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.3182 - mae: 1.3182 - val_loss: 3.5880 - val_mae: 3.5880\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.3109 - mae: 1.3109 - val_loss: 3.5623 - val_mae: 3.5623\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.3035 - mae: 1.3035 - val_loss: 3.5365 - val_mae: 3.5365\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.2961 - mae: 1.2961 - val_loss: 3.5106 - val_mae: 3.5106\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.2888 - mae: 1.2888 - val_loss: 3.4847 - val_mae: 3.4847\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.2813 - mae: 1.2813 - val_loss: 3.4588 - val_mae: 3.4588\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.2739 - mae: 1.2739 - val_loss: 3.4327 - val_mae: 3.4327\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.2665 - mae: 1.2665 - val_loss: 3.4067 - val_mae: 3.4067\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.2590 - mae: 1.2590 - val_loss: 3.3805 - val_mae: 3.3805\n",
      "Epoch 54/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.2516 - mae: 1.2516 - val_loss: 3.3543 - val_mae: 3.3543\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.2441 - mae: 1.2441 - val_loss: 3.3281 - val_mae: 3.3281\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.2366 - mae: 1.2366 - val_loss: 3.3018 - val_mae: 3.3018\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2291 - mae: 1.2291 - val_loss: 3.2754 - val_mae: 3.2754\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.2215 - mae: 1.2215 - val_loss: 3.2490 - val_mae: 3.2490\n",
      "Epoch 59/300\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2140 - mae: 1.2140 - val_loss: 3.2225 - val_mae: 3.2225\n",
      "Epoch 60/300\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2064 - mae: 1.2064 - val_loss: 3.1960 - val_mae: 3.1960\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1989 - mae: 1.1989 - val_loss: 3.1694 - val_mae: 3.1694\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1913 - mae: 1.1913 - val_loss: 3.1427 - val_mae: 3.1427\n",
      "Epoch 63/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1836 - mae: 1.1836 - val_loss: 3.1160 - val_mae: 3.1160\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1760 - mae: 1.1760 - val_loss: 3.0892 - val_mae: 3.0892\n",
      "Epoch 65/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1683 - mae: 1.1683 - val_loss: 3.0623 - val_mae: 3.0623\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.1607 - mae: 1.1607 - val_loss: 3.0354 - val_mae: 3.0354\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.1530 - mae: 1.1530 - val_loss: 3.0085 - val_mae: 3.0085\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.1453 - mae: 1.1453 - val_loss: 2.9814 - val_mae: 2.9814\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1375 - mae: 1.1375 - val_loss: 2.9543 - val_mae: 2.9543\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1298 - mae: 1.1298 - val_loss: 2.9271 - val_mae: 2.9271\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1220 - mae: 1.1220 - val_loss: 2.8999 - val_mae: 2.8999\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1143 - mae: 1.1143 - val_loss: 2.8726 - val_mae: 2.8726\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.1065 - mae: 1.1065 - val_loss: 2.8452 - val_mae: 2.8452\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.0986 - mae: 1.0986 - val_loss: 2.8178 - val_mae: 2.8178\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.0908 - mae: 1.0908 - val_loss: 2.7903 - val_mae: 2.7903\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.0829 - mae: 1.0829 - val_loss: 2.7627 - val_mae: 2.7627\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.0751 - mae: 1.0751 - val_loss: 2.7351 - val_mae: 2.7351\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.0672 - mae: 1.0672 - val_loss: 2.7074 - val_mae: 2.7074\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.0592 - mae: 1.0592 - val_loss: 2.6796 - val_mae: 2.6796\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.0513 - mae: 1.0513 - val_loss: 2.6517 - val_mae: 2.6517\n",
      "Epoch 81/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.0433 - mae: 1.0433 - val_loss: 2.6238 - val_mae: 2.6238\n",
      "Epoch 82/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.0354 - mae: 1.0354 - val_loss: 2.5958 - val_mae: 2.5958\n",
      "Epoch 83/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.0274 - mae: 1.0274 - val_loss: 2.5677 - val_mae: 2.5677\n",
      "Epoch 84/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.0194 - mae: 1.0194 - val_loss: 2.5396 - val_mae: 2.5396\n",
      "Epoch 85/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.0113 - mae: 1.0113 - val_loss: 2.5114 - val_mae: 2.5114\n",
      "Epoch 86/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.0033 - mae: 1.0033 - val_loss: 2.4831 - val_mae: 2.4831\n",
      "Epoch 87/300\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.9976 - mae: 0.9976 - val_loss: 2.4606 - val_mae: 2.4606\n",
      "Epoch 88/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.9914 - mae: 0.9914 - val_loss: 2.4408 - val_mae: 2.4408\n",
      "Epoch 89/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.9847 - mae: 0.9847 - val_loss: 2.4230 - val_mae: 2.4230\n",
      "Epoch 90/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.9775 - mae: 0.9775 - val_loss: 2.4069 - val_mae: 2.4069\n",
      "Epoch 91/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.9700 - mae: 0.9700 - val_loss: 2.3922 - val_mae: 2.3922\n",
      "Epoch 92/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.9624 - mae: 0.9624 - val_loss: 2.3786 - val_mae: 2.3786\n",
      "Epoch 93/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.9547 - mae: 0.9547 - val_loss: 2.3660 - val_mae: 2.3660\n",
      "Epoch 94/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.9468 - mae: 0.9468 - val_loss: 2.3542 - val_mae: 2.3542\n",
      "Epoch 95/300\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.9405 - mae: 0.9405 - val_loss: 2.3406 - val_mae: 2.3406\n",
      "Epoch 96/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.9341 - mae: 0.9341 - val_loss: 2.3254 - val_mae: 2.3254\n",
      "Epoch 97/300\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.9275 - mae: 0.9275 - val_loss: 2.3087 - val_mae: 2.3087\n",
      "Epoch 98/300\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.9206 - mae: 0.9206 - val_loss: 2.2907 - val_mae: 2.2907\n",
      "Epoch 99/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.9136 - mae: 0.9136 - val_loss: 2.2715 - val_mae: 2.2715\n",
      "Epoch 100/300\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.9064 - mae: 0.9064 - val_loss: 2.2511 - val_mae: 2.2511\n",
      "Epoch 101/300\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.8990 - mae: 0.8990 - val_loss: 2.2298 - val_mae: 2.2298\n",
      "Epoch 102/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.8915 - mae: 0.8915 - val_loss: 2.2075 - val_mae: 2.2075\n",
      "Epoch 103/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.8850 - mae: 0.8850 - val_loss: 2.1873 - val_mae: 2.1873\n",
      "Epoch 104/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.8785 - mae: 0.8785 - val_loss: 2.1688 - val_mae: 2.1688\n",
      "Epoch 105/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.8717 - mae: 0.8717 - val_loss: 2.1517 - val_mae: 2.1517\n",
      "Epoch 106/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.8647 - mae: 0.8647 - val_loss: 2.1359 - val_mae: 2.1359\n",
      "Epoch 107/300\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.8575 - mae: 0.8575 - val_loss: 2.1212 - val_mae: 2.1212\n",
      "Epoch 108/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.8502 - mae: 0.8502 - val_loss: 2.1074 - val_mae: 2.1074\n",
      "Epoch 109/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.8428 - mae: 0.8428 - val_loss: 2.0918 - val_mae: 2.0918\n",
      "Epoch 110/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.8362 - mae: 0.8362 - val_loss: 2.0747 - val_mae: 2.0747\n",
      "Epoch 111/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.8292 - mae: 0.8292 - val_loss: 2.0561 - val_mae: 2.0561\n",
      "Epoch 112/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.8221 - mae: 0.8221 - val_loss: 2.0362 - val_mae: 2.0362\n",
      "Epoch 113/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.8151 - mae: 0.8151 - val_loss: 2.0178 - val_mae: 2.0178\n",
      "Epoch 114/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.8082 - mae: 0.8082 - val_loss: 2.0006 - val_mae: 2.0006\n",
      "Epoch 115/300\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.8012 - mae: 0.8012 - val_loss: 1.9846 - val_mae: 1.9846\n",
      "Epoch 116/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.7940 - mae: 0.7940 - val_loss: 1.9695 - val_mae: 1.9695\n",
      "Epoch 117/300\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.7873 - mae: 0.7873 - val_loss: 1.9527 - val_mae: 1.9527\n",
      "Epoch 118/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7804 - mae: 0.7804 - val_loss: 1.9343 - val_mae: 1.9343\n",
      "Epoch 119/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7732 - mae: 0.7732 - val_loss: 1.9146 - val_mae: 1.9146\n",
      "Epoch 120/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.7659 - mae: 0.7659 - val_loss: 1.8961 - val_mae: 1.8961\n",
      "Epoch 121/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7589 - mae: 0.7589 - val_loss: 1.8788 - val_mae: 1.8788\n",
      "Epoch 122/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.7518 - mae: 0.7518 - val_loss: 1.8626 - val_mae: 1.8626\n",
      "Epoch 123/300\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.7448 - mae: 0.7448 - val_loss: 1.8447 - val_mae: 1.8447\n",
      "Epoch 124/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.7377 - mae: 0.7377 - val_loss: 1.8252 - val_mae: 1.8252\n",
      "Epoch 125/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step - loss: 0.7307 - mae: 0.7307 - val_loss: 1.8070 - val_mae: 1.8070\n",
      "Epoch 126/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.7236 - mae: 0.7236 - val_loss: 1.7898 - val_mae: 1.7898\n",
      "Epoch 127/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.7164 - mae: 0.7164 - val_loss: 1.7736 - val_mae: 1.7736\n",
      "Epoch 128/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.7093 - mae: 0.7093 - val_loss: 1.7557 - val_mae: 1.7557\n",
      "Epoch 129/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.7021 - mae: 0.7021 - val_loss: 1.7361 - val_mae: 1.7361\n",
      "Epoch 130/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.6950 - mae: 0.6950 - val_loss: 1.7177 - val_mae: 1.7177\n",
      "Epoch 131/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6879 - mae: 0.6879 - val_loss: 1.7004 - val_mae: 1.7004\n",
      "Epoch 132/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6806 - mae: 0.6806 - val_loss: 1.6839 - val_mae: 1.6839\n",
      "Epoch 133/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6734 - mae: 0.6734 - val_loss: 1.6657 - val_mae: 1.6657\n",
      "Epoch 134/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6662 - mae: 0.6662 - val_loss: 1.6458 - val_mae: 1.6458\n",
      "Epoch 135/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.6590 - mae: 0.6590 - val_loss: 1.6271 - val_mae: 1.6271\n",
      "Epoch 136/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6518 - mae: 0.6518 - val_loss: 1.6094 - val_mae: 1.6094\n",
      "Epoch 137/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6445 - mae: 0.6445 - val_loss: 1.5926 - val_mae: 1.5926\n",
      "Epoch 138/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.6371 - mae: 0.6371 - val_loss: 1.5765 - val_mae: 1.5765\n",
      "Epoch 139/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.6301 - mae: 0.6301 - val_loss: 1.5585 - val_mae: 1.5585\n",
      "Epoch 140/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.6229 - mae: 0.6229 - val_loss: 1.5388 - val_mae: 1.5388\n",
      "Epoch 141/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6153 - mae: 0.6153 - val_loss: 1.5175 - val_mae: 1.5175\n",
      "Epoch 142/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6080 - mae: 0.6080 - val_loss: 1.4974 - val_mae: 1.4974\n",
      "Epoch 143/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6009 - mae: 0.6009 - val_loss: 1.4784 - val_mae: 1.4784\n",
      "Epoch 144/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.5935 - mae: 0.5935 - val_loss: 1.4604 - val_mae: 1.4604\n",
      "Epoch 145/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.5861 - mae: 0.5861 - val_loss: 1.4431 - val_mae: 1.4431\n",
      "Epoch 146/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.5786 - mae: 0.5786 - val_loss: 1.4266 - val_mae: 1.4266\n",
      "Epoch 147/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.5709 - mae: 0.5709 - val_loss: 1.4107 - val_mae: 1.4107\n",
      "Epoch 148/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5638 - mae: 0.5638 - val_loss: 1.3927 - val_mae: 1.3927\n",
      "Epoch 149/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.5565 - mae: 0.5565 - val_loss: 1.3730 - val_mae: 1.3730\n",
      "Epoch 150/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.5488 - mae: 0.5488 - val_loss: 1.3515 - val_mae: 1.3515\n",
      "Epoch 151/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.5410 - mae: 0.5410 - val_loss: 1.3311 - val_mae: 1.3311\n",
      "Epoch 152/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.5337 - mae: 0.5337 - val_loss: 1.3118 - val_mae: 1.3118\n",
      "Epoch 153/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.5262 - mae: 0.5262 - val_loss: 1.2933 - val_mae: 1.2933\n",
      "Epoch 154/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.5186 - mae: 0.5186 - val_loss: 1.2755 - val_mae: 1.2755\n",
      "Epoch 155/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5109 - mae: 0.5109 - val_loss: 1.2585 - val_mae: 1.2585\n",
      "Epoch 156/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5033 - mae: 0.5033 - val_loss: 1.2394 - val_mae: 1.2394\n",
      "Epoch 157/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.4956 - mae: 0.4956 - val_loss: 1.2184 - val_mae: 1.2184\n",
      "Epoch 158/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4880 - mae: 0.4880 - val_loss: 1.1985 - val_mae: 1.1985\n",
      "Epoch 159/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4804 - mae: 0.4804 - val_loss: 1.1794 - val_mae: 1.1794\n",
      "Epoch 160/300\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.4727 - mae: 0.4727 - val_loss: 1.1610 - val_mae: 1.1610\n",
      "Epoch 161/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.4649 - mae: 0.4649 - val_loss: 1.1434 - val_mae: 1.1434\n",
      "Epoch 162/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.4572 - mae: 0.4572 - val_loss: 1.1236 - val_mae: 1.1236\n",
      "Epoch 163/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.4494 - mae: 0.4494 - val_loss: 1.1021 - val_mae: 1.1021\n",
      "Epoch 164/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.4417 - mae: 0.4417 - val_loss: 1.0814 - val_mae: 1.0814\n",
      "Epoch 165/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.4340 - mae: 0.4340 - val_loss: 1.0617 - val_mae: 1.0617\n",
      "Epoch 166/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.4262 - mae: 0.4262 - val_loss: 1.0427 - val_mae: 1.0427\n",
      "Epoch 167/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.4183 - mae: 0.4183 - val_loss: 1.0244 - val_mae: 1.0244\n",
      "Epoch 168/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.4103 - mae: 0.4103 - val_loss: 1.0067 - val_mae: 1.0067\n",
      "Epoch 169/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.4025 - mae: 0.4025 - val_loss: 0.9868 - val_mae: 0.9868\n",
      "Epoch 170/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3946 - mae: 0.3946 - val_loss: 0.9650 - val_mae: 0.9650\n",
      "Epoch 171/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3866 - mae: 0.3866 - val_loss: 0.9442 - val_mae: 0.9442\n",
      "Epoch 172/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3788 - mae: 0.3788 - val_loss: 0.9241 - val_mae: 0.9241\n",
      "Epoch 173/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3708 - mae: 0.3708 - val_loss: 0.9048 - val_mae: 0.9048\n",
      "Epoch 174/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3627 - mae: 0.3627 - val_loss: 0.8861 - val_mae: 0.8861\n",
      "Epoch 175/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3546 - mae: 0.3546 - val_loss: 0.8679 - val_mae: 0.8679\n",
      "Epoch 176/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3468 - mae: 0.3468 - val_loss: 0.8475 - val_mae: 0.8475\n",
      "Epoch 177/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3387 - mae: 0.3387 - val_loss: 0.8252 - val_mae: 0.8252\n",
      "Epoch 178/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3304 - mae: 0.3304 - val_loss: 0.8038 - val_mae: 0.8038\n",
      "Epoch 179/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3224 - mae: 0.3224 - val_loss: 0.7831 - val_mae: 0.7831\n",
      "Epoch 180/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3143 - mae: 0.3143 - val_loss: 0.7632 - val_mae: 0.7632\n",
      "Epoch 181/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3061 - mae: 0.3061 - val_loss: 0.7439 - val_mae: 0.7439\n",
      "Epoch 182/300\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.2978 - mae: 0.2978 - val_loss: 0.7251 - val_mae: 0.7251\n",
      "Epoch 183/300\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.2898 - mae: 0.2898 - val_loss: 0.7041 - val_mae: 0.7041\n",
      "Epoch 184/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.2815 - mae: 0.2815 - val_loss: 0.6811 - val_mae: 0.6811\n",
      "Epoch 185/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.2731 - mae: 0.2731 - val_loss: 0.6590 - val_mae: 0.6590\n",
      "Epoch 186/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.2649 - mae: 0.2649 - val_loss: 0.6377 - val_mae: 0.6377\n",
      "Epoch 187/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2566 - mae: 0.2566 - val_loss: 0.6171 - val_mae: 0.6171\n",
      "Epoch 188/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.2483 - mae: 0.2483 - val_loss: 0.5971 - val_mae: 0.5971\n",
      "Epoch 189/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.2398 - mae: 0.2398 - val_loss: 0.5776 - val_mae: 0.5776\n",
      "Epoch 190/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.2313 - mae: 0.2313 - val_loss: 0.5587 - val_mae: 0.5587\n",
      "Epoch 191/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.2232 - mae: 0.2232 - val_loss: 0.5373 - val_mae: 0.5373\n",
      "Epoch 192/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.2147 - mae: 0.2147 - val_loss: 0.5139 - val_mae: 0.5139\n",
      "Epoch 193/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.2060 - mae: 0.2060 - val_loss: 0.4914 - val_mae: 0.4914\n",
      "Epoch 194/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1976 - mae: 0.1976 - val_loss: 0.4696 - val_mae: 0.4696\n",
      "Epoch 195/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1891 - mae: 0.1891 - val_loss: 0.4485 - val_mae: 0.4485\n",
      "Epoch 196/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.1805 - mae: 0.1805 - val_loss: 0.4279 - val_mae: 0.4279\n",
      "Epoch 197/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1719 - mae: 0.1719 - val_loss: 0.4079 - val_mae: 0.4079\n",
      "Epoch 198/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1632 - mae: 0.1632 - val_loss: 0.3884 - val_mae: 0.3884\n",
      "Epoch 199/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.1549 - mae: 0.1549 - val_loss: 0.3664 - val_mae: 0.3664\n",
      "Epoch 200/300\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.1462 - mae: 0.1462 - val_loss: 0.3423 - val_mae: 0.3423\n",
      "Epoch 201/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.1372 - mae: 0.1372 - val_loss: 0.3191 - val_mae: 0.3191\n",
      "Epoch 202/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1286 - mae: 0.1286 - val_loss: 0.2965 - val_mae: 0.2965\n",
      "Epoch 203/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1199 - mae: 0.1199 - val_loss: 0.2747 - val_mae: 0.2747\n",
      "Epoch 204/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1111 - mae: 0.1111 - val_loss: 0.2534 - val_mae: 0.2534\n",
      "Epoch 205/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1023 - mae: 0.1023 - val_loss: 0.2327 - val_mae: 0.2327\n",
      "Epoch 206/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0934 - mae: 0.0934 - val_loss: 0.2124 - val_mae: 0.2124\n",
      "Epoch 207/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0847 - mae: 0.0847 - val_loss: 0.1897 - val_mae: 0.1897\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0757 - mae: 0.0757 - val_loss: 0.1647 - val_mae: 0.1647\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0668 - mae: 0.0668 - val_loss: 0.1407 - val_mae: 0.1407\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0580 - mae: 0.0580 - val_loss: 0.1173 - val_mae: 0.1173\n",
      "Epoch 211/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0490 - mae: 0.0490 - val_loss: 0.0947 - val_mae: 0.0947\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0401 - mae: 0.0401 - val_loss: 0.0726 - val_mae: 0.0726\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0310 - mae: 0.0310 - val_loss: 0.0511 - val_mae: 0.0511\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0219 - mae: 0.0219 - val_loss: 0.0300 - val_mae: 0.0300\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0127 - mae: 0.0127 - val_loss: 0.0093 - val_mae: 0.0093\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0140 - val_mae: 0.0140\n",
      "Epoch 217/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0057 - mae: 0.0057 - val_loss: 0.0333 - val_mae: 0.0333\n",
      "Epoch 218/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0131 - mae: 0.0131 - val_loss: 0.0459 - val_mae: 0.0459\n",
      "Epoch 219/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0186 - mae: 0.0186 - val_loss: 0.0557 - val_mae: 0.0557\n",
      "Epoch 220/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0628 - val_mae: 0.0628\n",
      "Epoch 221/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0256 - mae: 0.0256 - val_loss: 0.0676 - val_mae: 0.0676\n",
      "Epoch 222/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0271 - mae: 0.0271 - val_loss: 0.0703 - val_mae: 0.0703\n",
      "Epoch 223/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0278 - mae: 0.0278 - val_loss: 0.0679 - val_mae: 0.0679\n",
      "Epoch 224/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0271 - mae: 0.0271 - val_loss: 0.0611 - val_mae: 0.0611\n",
      "Epoch 225/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0257 - mae: 0.0257 - val_loss: 0.0533 - val_mae: 0.0533\n",
      "Epoch 226/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0447 - val_mae: 0.0447\n",
      "Epoch 227/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0208 - mae: 0.0208 - val_loss: 0.0352 - val_mae: 0.0352\n",
      "Epoch 228/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0173 - mae: 0.0173 - val_loss: 0.0251 - val_mae: 0.0251\n",
      "Epoch 229/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0132 - mae: 0.0132 - val_loss: 0.0143 - val_mae: 0.0143\n",
      "Epoch 230/300\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0086 - mae: 0.0086 - val_loss: 0.0028 - val_mae: 0.0028\n",
      "Epoch 231/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0039 - val_mae: 0.0039\n",
      "Epoch 232/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0018 - mae: 0.0018 - val_loss: 0.0083 - val_mae: 0.0083\n",
      "Epoch 233/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0056 - mae: 0.0056 - val_loss: 0.0106 - val_mae: 0.0106\n",
      "Epoch 234/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0081 - mae: 0.0081 - val_loss: 0.0110 - val_mae: 0.0110\n",
      "Epoch 235/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0093 - mae: 0.0093 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 236/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0096 - mae: 0.0096 - val_loss: 0.0067 - val_mae: 0.0067\n",
      "Epoch 237/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0095 - mae: 0.0095 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 238/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0075 - mae: 0.0075 - val_loss: 0.0068 - val_mae: 0.0068\n",
      "Epoch 239/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0054 - mae: 0.0054 - val_loss: 0.0043 - val_mae: 0.0043\n",
      "Epoch 240/300\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0026 - mae: 0.0026 - val_loss: 3.2258e-04 - val_mae: 3.2258e-04\n",
      "Epoch 241/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0012 - mae: 0.0012 - val_loss: 0.0069 - val_mae: 0.0069\n",
      "Epoch 242/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0033 - mae: 0.0033 - val_loss: 0.0116 - val_mae: 0.0116\n",
      "Epoch 243/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0046 - mae: 0.0046 - val_loss: 0.0112 - val_mae: 0.0112\n",
      "Epoch 244/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0049 - mae: 0.0049 - val_loss: 0.0091 - val_mae: 0.0091\n",
      "Epoch 245/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0043 - mae: 0.0043 - val_loss: 0.0054 - val_mae: 0.0054\n",
      "Epoch 246/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0029 - mae: 0.0029 - val_loss: 2.7990e-04 - val_mae: 2.7990e-04\n",
      "Epoch 247/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7.7845e-04 - mae: 7.7845e-04 - val_loss: 8.3828e-04 - val_mae: 8.3828e-04\n",
      "Epoch 248/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0023 - mae: 0.0023 - val_loss: 0.0054 - val_mae: 0.0054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0036 - mae: 0.0036 - val_loss: 0.0077 - val_mae: 0.0077\n",
      "Epoch 250/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0043 - mae: 0.0043 - val_loss: 0.0079 - val_mae: 0.0079\n",
      "Epoch 251/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0040 - mae: 0.0040 - val_loss: 0.0064 - val_mae: 0.0064\n",
      "Epoch 252/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0028 - mae: 0.0028 - val_loss: 0.0032 - val_mae: 0.0032\n",
      "Epoch 253/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0011 - mae: 0.0011 - val_loss: 0.0044 - val_mae: 0.0044\n",
      "Epoch 254/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0017 - mae: 0.0017 - val_loss: 0.0065 - val_mae: 0.0065\n",
      "Epoch 255/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0032 - mae: 0.0032 - val_loss: 0.0066 - val_mae: 0.0066\n",
      "Epoch 256/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0037 - mae: 0.0037 - val_loss: 0.0049 - val_mae: 0.0049\n",
      "Epoch 257/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0032 - mae: 0.0032 - val_loss: 0.0016 - val_mae: 0.0016\n",
      "Epoch 258/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0019 - mae: 0.0019 - val_loss: 0.0020 - val_mae: 0.0020\n",
      "Epoch 259/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6.3445e-04 - mae: 6.3445e-04 - val_loss: 0.0028 - val_mae: 0.0028\n",
      "Epoch 260/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0018 - mae: 0.0018 - val_loss: 0.0054 - val_mae: 0.0054\n",
      "Epoch 261/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0026 - mae: 0.0026 - val_loss: 0.0059 - val_mae: 0.0059\n",
      "Epoch 262/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0025 - mae: 0.0025 - val_loss: 0.0046 - val_mae: 0.0046\n",
      "Epoch 263/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0017 - mae: 0.0017 - val_loss: 0.0013 - val_mae: 0.0013\n",
      "Epoch 264/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.0516e-04 - mae: 4.0516e-04 - val_loss: 0.0020 - val_mae: 0.0020\n",
      "Epoch 265/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.7968e-04 - mae: 9.7968e-04 - val_loss: 7.1144e-04 - val_mae: 7.1144e-04\n",
      "Epoch 266/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7.8228e-04 - mae: 7.8228e-04 - val_loss: 0.0030 - val_mae: 0.0030\n",
      "Epoch 267/300\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 9.9003e-04 - mae: 9.9003e-04 - val_loss: 2.3508e-04 - val_mae: 2.3508e-04\n",
      "Epoch 268/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0012 - mae: 0.0012 - val_loss: 0.0064 - val_mae: 0.0064\n",
      "Epoch 269/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0020 - mae: 0.0020 - val_loss: 0.0073 - val_mae: 0.0073\n",
      "Epoch 270/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0022 - mae: 0.0022 - val_loss: 0.0030 - val_mae: 0.0030\n",
      "Epoch 271/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9.6086e-04 - mae: 9.6086e-04 - val_loss: 0.0061 - val_mae: 0.0061\n",
      "Epoch 272/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0018 - mae: 0.0018 - val_loss: 0.0096 - val_mae: 0.0096\n",
      "Epoch 273/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0029 - mae: 0.0029 - val_loss: 0.0076 - val_mae: 0.0076\n",
      "Epoch 274/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0023 - mae: 0.0023 - val_loss: 6.5446e-04 - val_mae: 6.5446e-04\n",
      "Epoch 275/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.7867e-04 - mae: 2.7867e-04 - val_loss: 0.0074 - val_mae: 0.0074\n",
      "Epoch 276/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0021 - mae: 0.0021 - val_loss: 0.0095 - val_mae: 0.0095\n",
      "Epoch 277/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0029 - mae: 0.0029 - val_loss: 0.0067 - val_mae: 0.0067\n",
      "Epoch 278/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0023 - mae: 0.0023 - val_loss: 2.6655e-04 - val_mae: 2.6655e-04\n",
      "Epoch 279/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0016 - mae: 0.0016 - val_loss: 0.0033 - val_mae: 0.0033\n",
      "Epoch 280/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0011 - mae: 0.0011 - val_loss: 8.9097e-04 - val_mae: 8.9097e-04\n",
      "Epoch 281/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 8.7538e-04 - mae: 8.7538e-04 - val_loss: 0.0030 - val_mae: 0.0030\n",
      "Epoch 282/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0020 - mae: 0.0020 - val_loss: 0.0033 - val_mae: 0.0033\n",
      "Epoch 283/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0012 - mae: 0.0012 - val_loss: 2.5392e-04 - val_mae: 2.5392e-04\n",
      "Epoch 284/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0013 - mae: 0.0013 - val_loss: 7.9727e-04 - val_mae: 7.9727e-04\n",
      "Epoch 285/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0017 - mae: 0.0017 - val_loss: 0.0045 - val_mae: 0.0045\n",
      "Epoch 286/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0016 - mae: 0.0016 - val_loss: 0.0033 - val_mae: 0.0033\n",
      "Epoch 287/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9.8138e-04 - mae: 9.8138e-04 - val_loss: 0.0024 - val_mae: 0.0024\n",
      "Epoch 288/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.7941e-04 - mae: 9.7941e-04 - val_loss: 0.0057 - val_mae: 0.0057\n",
      "Epoch 289/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0019 - mae: 0.0019 - val_loss: 0.0041 - val_mae: 0.0041\n",
      "Epoch 290/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0015 - mae: 0.0015 - val_loss: 0.0019 - val_mae: 0.0019\n",
      "Epoch 291/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.5537e-04 - mae: 9.5537e-04 - val_loss: 0.0041 - val_mae: 0.0041\n",
      "Epoch 292/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0014 - mae: 0.0014 - val_loss: 0.0016 - val_mae: 0.0016\n",
      "Epoch 293/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0017 - mae: 0.0017 - val_loss: 0.0024 - val_mae: 0.0024\n",
      "Epoch 294/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0017 - mae: 0.0017 - val_loss: 0.0014 - val_mae: 0.0014\n",
      "Epoch 295/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7.9438e-04 - mae: 7.9438e-04 - val_loss: 0.0013 - val_mae: 0.0013\n",
      "Epoch 296/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 8.0808e-04 - mae: 8.0808e-04 - val_loss: 0.0020 - val_mae: 0.0020\n",
      "Epoch 297/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0014 - mae: 0.0014 - val_loss: 7.2169e-04 - val_mae: 7.2169e-04\n",
      "Epoch 298/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0013 - mae: 0.0013 - val_loss: 0.0028 - val_mae: 0.0028\n",
      "Epoch 299/300\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.0886e-04 - mae: 8.0886e-04 - val_loss: 2.1315e-04 - val_mae: 2.1315e-04\n",
      "Epoch 300/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0014 - mae: 0.0014 - val_loss: 0.0055 - val_mae: 0.0055\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    model = solution_model()\n",
    "    model.save(\"mymodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14c64045",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mymodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7376d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.models.load_model('mymodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28a2cc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step - loss: 9.8324e-04 - mae: 9.8324e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0009832382202148438, 0.0009832382202148438]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
    "model.evaluate(tf.expand_dims(xs,axis=-1),\n",
    "                                  tf.expand_dims(ys,axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3049fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.997992  ],\n",
       "       [-0.99841833],\n",
       "       [ 1.001155  ],\n",
       "       [ 3.0007284 ],\n",
       "       [ 5.0003023 ],\n",
       "       [ 6.9998755 ]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00612f97",
   "metadata": {},
   "source": [
    "### Category 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03de40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "def create_checkpoint_callback(model_name):\n",
    "    return tf.keras.callbacks.ModelCheckpoint(filepath=f\"{model_name}/checkpoint.ckpt\",\n",
    "                                              #  monitor=\"val_accuracy\",\n",
    "                                              save_best_only=True,\n",
    "                                              save_weights_only=True,\n",
    "                                              save_freq=\"epoch\")\n",
    "\n",
    "# Note that you'll need to save your model as a .h5 like this.\n",
    "# When you press the Submit and Test button, your saved .h5 model will\n",
    "# be sent to the testing infrastructure for scoring\n",
    "# and the score will be returned to you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cad05243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution_model():\n",
    "    #url = 'https://storage.googleapis.com/download.tensorflow.org/data/rps.zip'\n",
    "    #urllib.request.urlretrieve(url, 'rps.zip')\n",
    "    #local_zip = 'rps.zip'\n",
    "    #zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "    #zip_ref.extractall('tmp/')\n",
    "    #zip_ref.close()\n",
    "\n",
    "\n",
    "    TRAINING_DIR = \"tmp/rps/\"\n",
    "    training_datagen = ImageDataGenerator(\n",
    "      rescale = 1./255,\n",
    "\t    rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest',\n",
    "    validation_split=0.2)\n",
    "\n",
    "    train_generator = training_datagen.flow_from_directory(\n",
    "\tTRAINING_DIR,\n",
    "\ttarget_size=(150,150),\n",
    "\tclass_mode='categorical',\n",
    "  batch_size=126\n",
    ")# YOUR CODE HERE\n",
    "\n",
    "    validation_generator = training_datagen.flow_from_directory(\n",
    "        TRAINING_DIR,\n",
    "        target_size=(150, 150),\n",
    "        class_mode='categorical',\n",
    "        batch_size=126\n",
    "    )\n",
    "    model = tf.keras.models.Sequential([\n",
    "    # YOUR CODE HERE, BUT END WITH A 3 Neuron Dense, activated by softmax\n",
    "        # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
    "        # This is the first convolution\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        # The second convolution\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        # The third convolution\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        # The fourth convolution\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        # Flatten the results to feed into a DNN\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # 512 neuron hidden layer\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(train_generator, epochs=25, steps_per_epoch=20, validation_data=validation_generator, verbose=1,\n",
    "                        validation_steps=3)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8414945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    model = solution_model()\n",
    "    model.save(\"mymodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e494d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/rps.zip'\n",
    "urllib.request.urlretrieve(url, 'rps.zip')\n",
    "local_zip = 'rps.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('tmp/')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec35fdb",
   "metadata": {},
   "source": [
    "# category 4 NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c707a0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sarcasm.json', <http.client.HTTPMessage at 0x20edfdf0640>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
    "urllib.request.urlretrieve(url, 'sarcasm.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c93733b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26704</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/american-...</td>\n",
       "      <td>american politics in moral free-fall</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26705</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/americas-...</td>\n",
       "      <td>america's best 20 hikes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26706</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/reparatio...</td>\n",
       "      <td>reparations and obama</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26707</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/israeli-b...</td>\n",
       "      <td>israeli ban targeting boycott supporters raise...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26708</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/gourmet-g...</td>\n",
       "      <td>gourmet gifts for the foodie 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26709 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            article_link  \\\n",
       "0      https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1      https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2      https://local.theonion.com/mom-starting-to-fea...   \n",
       "3      https://politics.theonion.com/boehner-just-wan...   \n",
       "4      https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "...                                                  ...   \n",
       "26704  https://www.huffingtonpost.com/entry/american-...   \n",
       "26705  https://www.huffingtonpost.com/entry/americas-...   \n",
       "26706  https://www.huffingtonpost.com/entry/reparatio...   \n",
       "26707  https://www.huffingtonpost.com/entry/israeli-b...   \n",
       "26708  https://www.huffingtonpost.com/entry/gourmet-g...   \n",
       "\n",
       "                                                headline  is_sarcastic  \n",
       "0      former versace store clerk sues over secret 'b...             0  \n",
       "1      the 'roseanne' revival catches up to our thorn...             0  \n",
       "2      mom starting to fear son's web series closest ...             1  \n",
       "3      boehner just wants wife to listen, not come up...             1  \n",
       "4      j.k. rowling wishes snape happy birthday in th...             0  \n",
       "...                                                  ...           ...  \n",
       "26704               american politics in moral free-fall             0  \n",
       "26705                            america's best 20 hikes             0  \n",
       "26706                              reparations and obama             0  \n",
       "26707  israeli ban targeting boycott supporters raise...             0  \n",
       "26708                  gourmet gifts for the foodie 2014             0  \n",
       "\n",
       "[26709 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_json('sarcasm.json')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d83add29",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e169bef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21367, 5342, 21367, 5342)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(data[\"headline\"].to_numpy(),\n",
    "                                                                            data[\"is_sarcastic\"].to_numpy(),\n",
    "                                                                            test_size=0.2,\n",
    "                                                                            random_state=42)\n",
    "\n",
    "len(train_sentences), len(val_sentences), len(train_labels), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0d6fa1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d228ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f4e08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "training_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(val_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a22118af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13660/1650364618.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"input_layer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_vectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs, name=\"model_LSTM\")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "# Fit the model\n",
    "model_history = model.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              epochs=10,\n",
    "                              callbacks=[create_checkpoint_callback(model.name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb244b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "668/668 [==============================] - 4s 6ms/step - loss: 0.4670 - accuracy: 0.7659 - val_loss: 0.3947 - val_accuracy: 0.8203\n",
      "Epoch 2/10\n",
      "668/668 [==============================] - 4s 6ms/step - loss: 0.3597 - accuracy: 0.8362 - val_loss: 0.3792 - val_accuracy: 0.8308\n",
      "Epoch 3/10\n",
      "668/668 [==============================] - 4s 6ms/step - loss: 0.3234 - accuracy: 0.8578 - val_loss: 0.3925 - val_accuracy: 0.8224\n",
      "Epoch 4/10\n",
      "668/668 [==============================] - 4s 6ms/step - loss: 0.2911 - accuracy: 0.8739 - val_loss: 0.3805 - val_accuracy: 0.8300\n",
      "Epoch 5/10\n",
      "668/668 [==============================] - 4s 6ms/step - loss: 0.2595 - accuracy: 0.8901 - val_loss: 0.4032 - val_accuracy: 0.8278\n",
      "Epoch 6/10\n",
      "668/668 [==============================] - 4s 6ms/step - loss: 0.2258 - accuracy: 0.9086 - val_loss: 0.4291 - val_accuracy: 0.8267\n",
      "Epoch 7/10\n",
      "668/668 [==============================] - 4s 6ms/step - loss: 0.1923 - accuracy: 0.9233 - val_loss: 0.4793 - val_accuracy: 0.8074\n",
      "Epoch 8/10\n",
      "668/668 [==============================] - 4s 6ms/step - loss: 0.1616 - accuracy: 0.9369 - val_loss: 0.5595 - val_accuracy: 0.8092\n",
      "Epoch 9/10\n",
      "668/668 [==============================] - 4s 6ms/step - loss: 0.1340 - accuracy: 0.9501 - val_loss: 0.6064 - val_accuracy: 0.8076\n",
      "Epoch 10/10\n",
      "668/668 [==============================] - 4s 6ms/step - loss: 0.1116 - accuracy: 0.9575 - val_loss: 0.6801 - val_accuracy: 0.8064\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "# Fit the model\n",
    "model_history = model.fit(training_padded,train_labels,\n",
    "                              validation_data=(testing_padded,val_labels),\n",
    "                              epochs=10,\n",
    "                              callbacks=[create_checkpoint_callback(model.name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b8a6978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def solution_model():\n",
    "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
    "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
    "\n",
    "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
    "    vocab_size = 1000\n",
    "    embedding_dim = 16\n",
    "    max_length = 120\n",
    "    trunc_type='post'\n",
    "    padding_type='post'\n",
    "    oov_tok = \"<OOV>\"\n",
    "    training_size = 20000\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    # YOUR CODE HERE\n",
    "    with open(\"sarcasm.json\", 'r') as f:\n",
    "        datastore = json.load(f)\n",
    "    for item in datastore:\n",
    "        sentences.append(item['headline'])\n",
    "        labels.append(item['is_sarcastic'])\n",
    "    training_sentences = sentences[0:training_size]\n",
    "    testing_sentences = sentences[training_size:]\n",
    "    training_labels = labels[0:training_size]\n",
    "    testing_labels = labels[training_size:]\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "    training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "    testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    training_padded = np.array(training_padded)\n",
    "    training_labels = np.array(training_labels)\n",
    "    testing_padded = np.array(testing_padded)\n",
    "    testing_labels = np.array(testing_labels)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "        tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        tf.keras.layers.Dense(24, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    num_epochs = 50\n",
    "    history = model.fit(training_padded, training_labels, epochs=num_epochs,\n",
    "                        validation_data=(testing_padded, testing_labels), verbose=1,\n",
    "                        callbacks=[create_checkpoint_callback(model.name)])\n",
    "    model.load_weights(\"nlp/checkpoint.ckpt\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12012d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "618/625 [============================>.] - ETA: 0s - loss: 0.4630 - accuracy: 0.7701"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13660/3723374814.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolution_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mymodel.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13660/1360486337.py\u001b[0m in \u001b[0;36msolution_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     history = model.fit(training_padded, training_labels, epochs=num_epochs,\n\u001b[0m\u001b[0;32m     53\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                         callbacks=[create_checkpoint_callback(model.name)])\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1418\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[1;32m-> 1420\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m   1421\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1714\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1715\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1716\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1717\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1718\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = solution_model()\n",
    "model.save(\"mymodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8de33380",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "labels=[]\n",
    "training_size=2000\n",
    "with open(\"sarcasm.json\", 'r') as f:\n",
    "        datastore = json.load(f)\n",
    "for item in datastore:\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb7e608c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db240a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fa51147",
   "metadata": {},
   "source": [
    "# Category 5 Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d87762b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip'\n",
    "urllib.request.urlretrieve(url, 'household_power.zip')\n",
    "with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3357b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers,Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D,Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pathlib\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28071f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateReducerCb(tf.keras.callbacks.Callback):\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    old_lr = self.model.optimizer.lr.read_value()\n",
    "    new_lr = old_lr * 0.99\n",
    "    print(\"\\nEpoch: {}. Reducing Learning Rate from {} to {}\".format(epoch, old_lr, new_lr))\n",
    "    self.model.optimizer.lr.assign(new_lr)\n",
    "    \n",
    "def create_checkpoint_callback(model_name):\n",
    "  return tf.keras.callbacks.ModelCheckpoint(filepath=f\"{model_name}/checkpoint.ckpt\",\n",
    "                                                          #  monitor=\"val_accuracy\",\n",
    "                                                           save_best_only=True,\n",
    "                                                           save_weights_only=True,\n",
    "                                                           save_freq=\"epoch\")\n",
    "\n",
    "es_cb = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f28adbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Global_active_power</th>\n",
       "      <th>Global_reactive_power</th>\n",
       "      <th>Voltage</th>\n",
       "      <th>Global_intensity</th>\n",
       "      <th>Sub_metering_1</th>\n",
       "      <th>Sub_metering_2</th>\n",
       "      <th>Sub_metering_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-12-16 17:24:00</th>\n",
       "      <td>4.216</td>\n",
       "      <td>0.418</td>\n",
       "      <td>234.84</td>\n",
       "      <td>18.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16 17:25:00</th>\n",
       "      <td>5.360</td>\n",
       "      <td>0.436</td>\n",
       "      <td>233.63</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16 17:26:00</th>\n",
       "      <td>5.374</td>\n",
       "      <td>0.498</td>\n",
       "      <td>233.29</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16 17:27:00</th>\n",
       "      <td>5.388</td>\n",
       "      <td>0.502</td>\n",
       "      <td>233.74</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16 17:28:00</th>\n",
       "      <td>3.666</td>\n",
       "      <td>0.528</td>\n",
       "      <td>235.68</td>\n",
       "      <td>15.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-14 17:19:00</th>\n",
       "      <td>0.636</td>\n",
       "      <td>0.140</td>\n",
       "      <td>241.16</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-14 17:20:00</th>\n",
       "      <td>0.552</td>\n",
       "      <td>0.000</td>\n",
       "      <td>240.46</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-14 17:21:00</th>\n",
       "      <td>0.538</td>\n",
       "      <td>0.000</td>\n",
       "      <td>239.74</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-14 17:22:00</th>\n",
       "      <td>0.524</td>\n",
       "      <td>0.000</td>\n",
       "      <td>241.08</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02-14 17:23:00</th>\n",
       "      <td>0.544</td>\n",
       "      <td>0.000</td>\n",
       "      <td>241.62</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86400 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Global_active_power  Global_reactive_power  Voltage  \\\n",
       "datetime                                                                   \n",
       "2006-12-16 17:24:00                4.216                  0.418   234.84   \n",
       "2006-12-16 17:25:00                5.360                  0.436   233.63   \n",
       "2006-12-16 17:26:00                5.374                  0.498   233.29   \n",
       "2006-12-16 17:27:00                5.388                  0.502   233.74   \n",
       "2006-12-16 17:28:00                3.666                  0.528   235.68   \n",
       "...                                  ...                    ...      ...   \n",
       "2007-02-14 17:19:00                0.636                  0.140   241.16   \n",
       "2007-02-14 17:20:00                0.552                  0.000   240.46   \n",
       "2007-02-14 17:21:00                0.538                  0.000   239.74   \n",
       "2007-02-14 17:22:00                0.524                  0.000   241.08   \n",
       "2007-02-14 17:23:00                0.544                  0.000   241.62   \n",
       "\n",
       "                     Global_intensity  Sub_metering_1  Sub_metering_2  \\\n",
       "datetime                                                                \n",
       "2006-12-16 17:24:00              18.4             0.0             1.0   \n",
       "2006-12-16 17:25:00              23.0             0.0             1.0   \n",
       "2006-12-16 17:26:00              23.0             0.0             2.0   \n",
       "2006-12-16 17:27:00              23.0             0.0             1.0   \n",
       "2006-12-16 17:28:00              15.8             0.0             1.0   \n",
       "...                               ...             ...             ...   \n",
       "2007-02-14 17:19:00               2.6             0.0             0.0   \n",
       "2007-02-14 17:20:00               2.2             0.0             0.0   \n",
       "2007-02-14 17:21:00               2.2             0.0             0.0   \n",
       "2007-02-14 17:22:00               2.2             0.0             0.0   \n",
       "2007-02-14 17:23:00               2.2             0.0             1.0   \n",
       "\n",
       "                     Sub_metering_3  \n",
       "datetime                             \n",
       "2006-12-16 17:24:00            17.0  \n",
       "2006-12-16 17:25:00            16.0  \n",
       "2006-12-16 17:26:00            17.0  \n",
       "2006-12-16 17:27:00            17.0  \n",
       "2006-12-16 17:28:00            17.0  \n",
       "...                             ...  \n",
       "2007-02-14 17:19:00             0.0  \n",
       "2007-02-14 17:20:00             0.0  \n",
       "2007-02-14 17:21:00             0.0  \n",
       "2007-02-14 17:22:00             0.0  \n",
       "2007-02-14 17:23:00             0.0  \n",
       "\n",
       "[86400 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('household_power_consumption.csv',index_col=['datetime'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25cd1d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_series(data, min, max):\n",
    "    data = data - min\n",
    "    data = data / max\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a5a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forecast(model, series, window_size):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "    ds = ds.batch(32).prefetch(1)\n",
    "    forecast = model.predict(ds)\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf75e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "343f52c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_FEATURES = len(df.columns)\n",
    "N_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faf45464",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values\n",
    "data = normalize_series(data, data.min(axis=0), data.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ad2b006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.433779</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.040366</td>\n",
       "      <td>0.435644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.557161</td>\n",
       "      <td>0.498856</td>\n",
       "      <td>0.035558</td>\n",
       "      <td>0.549505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.558671</td>\n",
       "      <td>0.569794</td>\n",
       "      <td>0.034207</td>\n",
       "      <td>0.549505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.560181</td>\n",
       "      <td>0.574371</td>\n",
       "      <td>0.035995</td>\n",
       "      <td>0.549505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.374461</td>\n",
       "      <td>0.604119</td>\n",
       "      <td>0.043703</td>\n",
       "      <td>0.371287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86395</th>\n",
       "      <td>0.047670</td>\n",
       "      <td>0.160183</td>\n",
       "      <td>0.065475</td>\n",
       "      <td>0.044554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86396</th>\n",
       "      <td>0.038611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062694</td>\n",
       "      <td>0.034653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86397</th>\n",
       "      <td>0.037101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059833</td>\n",
       "      <td>0.034653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86398</th>\n",
       "      <td>0.035591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065157</td>\n",
       "      <td>0.034653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86399</th>\n",
       "      <td>0.037748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067302</td>\n",
       "      <td>0.034653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86400 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3    4         5     6\n",
       "0      0.433779  0.478261  0.040366  0.435644  0.0  0.012821  0.85\n",
       "1      0.557161  0.498856  0.035558  0.549505  0.0  0.012821  0.80\n",
       "2      0.558671  0.569794  0.034207  0.549505  0.0  0.025641  0.85\n",
       "3      0.560181  0.574371  0.035995  0.549505  0.0  0.012821  0.85\n",
       "4      0.374461  0.604119  0.043703  0.371287  0.0  0.012821  0.85\n",
       "...         ...       ...       ...       ...  ...       ...   ...\n",
       "86395  0.047670  0.160183  0.065475  0.044554  0.0  0.000000  0.00\n",
       "86396  0.038611  0.000000  0.062694  0.034653  0.0  0.000000  0.00\n",
       "86397  0.037101  0.000000  0.059833  0.034653  0.0  0.000000  0.00\n",
       "86398  0.035591  0.000000  0.065157  0.034653  0.0  0.000000  0.00\n",
       "86399  0.037748  0.000000  0.067302  0.034653  0.0  0.012821  0.00\n",
       "\n",
       "[86400 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49cbc6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_TIME = int(len(data) * 0.5) # DO NOT CHANGE THIS\n",
    "x_train = data[:SPLIT_TIME]\n",
    "x_valid = data[SPLIT_TIME:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "519037bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0a072ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PAST = 24  # DO NOT CHANGE THIS\n",
    "\n",
    "    # Number of future time steps which are to be predicted.\n",
    "N_FUTURE = 24  # DO NOT CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "030e4063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
    "    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
    "    return ds.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "594b7c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHIFT = 1  # DO NOT CHANGE THIS\n",
    "\n",
    "    # Code to create windowed train and validation datasets.\n",
    "train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,\n",
    "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
    "                                 shift=SHIFT)\n",
    "valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,\n",
    "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
    "                                 shift=SHIFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd6275c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1349/1349 [==============================] - 54s 39ms/step - loss: 0.0649 - mae: 0.0649 - val_loss: 0.0563 - val_mae: 0.0563\n",
      "Epoch 2/10\n",
      "1349/1349 [==============================] - 62s 46ms/step - loss: 0.0524 - mae: 0.0524 - val_loss: 0.0504 - val_mae: 0.0504\n",
      "Epoch 3/10\n",
      "1349/1349 [==============================] - 59s 44ms/step - loss: 0.0485 - mae: 0.0485 - val_loss: 0.0476 - val_mae: 0.0476\n",
      "Epoch 4/10\n",
      "1349/1349 [==============================] - 59s 44ms/step - loss: 0.0475 - mae: 0.0475 - val_loss: 0.0471 - val_mae: 0.0471\n",
      "Epoch 5/10\n",
      "1349/1349 [==============================] - 65s 48ms/step - loss: 0.0466 - mae: 0.0466 - val_loss: 0.0469 - val_mae: 0.0469\n",
      "Epoch 6/10\n",
      "1349/1349 [==============================] - 63s 46ms/step - loss: 0.0458 - mae: 0.0458 - val_loss: 0.0462 - val_mae: 0.0462\n",
      "Epoch 7/10\n",
      "1349/1349 [==============================] - 60s 45ms/step - loss: 0.0453 - mae: 0.0453 - val_loss: 0.0452 - val_mae: 0.0452\n",
      "Epoch 8/10\n",
      "1349/1349 [==============================] - 66s 49ms/step - loss: 0.0450 - mae: 0.0450 - val_loss: 0.0449 - val_mae: 0.0449\n",
      "Epoch 9/10\n",
      "1349/1349 [==============================] - 59s 44ms/step - loss: 0.0444 - mae: 0.0444 - val_loss: 0.0445 - val_mae: 0.0445\n",
      "Epoch 10/10\n",
      "1349/1349 [==============================] - 57s 42ms/step - loss: 0.0440 - mae: 0.0440 - val_loss: 0.0443 - val_mae: 0.0443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x263ea58a430>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code to define your model.\n",
    "model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units = 200, activation = 'relu',return_sequences=True,\n",
    "                                                          input_shape=(N_PAST,N_FEATURES))),\n",
    "        # The input layer of your model must have an input shape of: (BATCH_SIZE, N_PAST = 24, N_FEATURES = 7)\n",
    "        # The model must have an output shape of: (BATCH_SIZE, N_FUTURE = 24, N_FEATURES = 7).\n",
    "        # Make sure that there are N_FEATURES = 7 neurons in the final dense layer since the model predicts 7 features.\n",
    "\n",
    "        # HINT: Bidirectional LSTMs may help boost your score. This is only a\n",
    "        tf.keras.layers.Dense(7)\n",
    "    ])\n",
    "\n",
    "    # Code to train and compile the model\n",
    "optimizer = tf.keras.optimizers.Adam() # YOUR CODE HERE\n",
    "model.compile(\n",
    "        # YOUR CODE HERE\n",
    "        loss=\"mae\",\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "model.fit(\n",
    "        # YOUR CODE HERE\n",
    "        train_set,\n",
    "        validation_data=valid_set,\n",
    "        epochs=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fba4e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Time.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f422f2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, None, 400)        332800    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 7)           2807      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 335,607\n",
      "Trainable params: 335,607\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a47d9550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=(TensorSpec(shape=(None, None, 7), dtype=tf.float64, name=None), TensorSpec(shape=(None, None, 7), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.take(0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "04039175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true, y_pred):\n",
    "    return np.mean(abs(y_true.ravel() - y_pred.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c9b1a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forecast(model, series, window_size, batch_size):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "    ds = ds.batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "    forecast = model.predict(ds)\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "882da741",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_forecast = model_forecast(model, data, N_PAST, BATCH_SIZE)\n",
    "rnn_forecast = rnn_forecast[SPLIT_TIME - N_PAST:-1, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "55c507cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = x_valid[:rnn_forecast.shape[0]]\n",
    "result = mae(x_valid, rnn_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9df4c5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3199098e-01,  1.2387662e-01,  6.2390335e-02,  1.2769710e-01,\n",
       "        2.8830499e-04, -3.7498173e-04,  9.1008127e-01], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_forecast[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f62a6277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302400"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
